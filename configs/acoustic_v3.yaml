bases: ["configs/base_v3.yaml"]

data:
  dictionaries:
    zh: "dictionaries/opencpop-extension.txt"
    ja: "dictionaries/japanese_dict_full.txt"
    en: "dictionaries/ds_cmudict-07b.txt"
  extra_phonemes: ["EP", "ja/cl"]
  merged_phoneme_groups:
    - ["zh/i", "ja/i", "en/iy"]
    - ["SP", "ja/cl"]
  sources:
    - raw_data_dir: "data/qixuan_v4/raw"
      speaker: qixuan
      spk_id: 0
      language: zh
      test_prefixes:
        - "DongWuSenLin_021"
        - "HaoXiangAiZheGeShiJieA_001"
    - raw_data_dir: "data/junninghua_v3_part1_glide/raw"
      speaker: junninghua
      spk_id: 1
      language: zh
    - raw_data_dir: "data/junninghua_v3_part2_glide/raw"
      speaker: junninghua
      spk_id: 1
      language: zh

binarizer:
  binary_data_dir: "data/qixuan_v4/binary/acoustic"
  num_workers: 4
  extractors:
    pitch_extraction:
      method: rmvpe
      model_path: "assets/rmvpe/model.pt"
      f0_min: 65
      f0_max: 1100
    harmonic_noise_separation:
      method: vr
      model_path: "assets/vr/model.pt"
  features:
    audio_sample_rate: 44100
    hop_size: 512
    fft_size: 2048
    win_size: 2048
    spectrogram:
      type: mel
      num_bins: 128
      fmin: 40
      fmax: 16000
    energy:
      enabled: true
      smooth_width: 0.12
    breathiness:
      enabled: true
      smooth_width: 0.12
    voicing:
      enabled: true
      smooth_width: 0.12
    tension:
      enabled: true
      smooth_width: 0.12
  augmentation:
    random_pitch_shifting:
      enabled: true
      range: [-5., 5.]
      scale: 0.75
    random_time_stretching:
      enabled: true
      range: [0.5, 2.]
      scale: 0.75

model:
  use_spk_id: true
  num_spk: 2
  hidden_size: 256
  linguistic_encoder:
    use_lang_id: true
    num_lang: 3
    arch: fs2
    kwargs:
      dropout: 0.1
      use_pos_embed: true
      num_layers: 4
      num_heads: 2
      ffn_kernel_size: 3
      ffn_act: gelu
      use_rope: true
      rel_pos: true
  embeddings:
    use_energy_embed: false
    use_breathiness_embed: false
    use_voicing_embed: false
    use_tension_embed: false
    use_key_shift_embed: true
    use_speed_embed: true
  normalization:
    spec_min: -12.
    spec_max: 0.
  spec_decoder:
    use_shallow_diffusion: true
    aux_decoder_grad: 0.1
    aux_decoder_arch: convnext
    aux_decoder_kwargs:
      num_channels: 512
      num_layers: 6
      kernel_size: 7
      dropout_rate: 0.1
    diffusion_type: reflow
    t_start: 0.4
    time_scale_factor: 1000
    sampling_algorithm: euler
    sampling_steps: 20
    backbone_arch: lynxnet
    backbone_kwargs:
      num_channels: 1024
      num_layers: 6
      kernel_size: 31
      dropout_rate: 0.0
      strong_cond: true

training:
  loss:
    spec_decoder:
      main_loss_type: L2
      main_loss_log_norm: false
      aux_loss_type: L1
  dataloader:
    max_batch_frames: 50000
    max_batch_size: 48
    max_val_batch_frames: 20000
    max_val_batch_size: 1
    frame_count_grid: 6
    num_workers: 4
    prefetch_factor: 2
  optimizer:
    cls: torch.optim.AdamW
    kwargs:
      lr: 0.0006
      betas: [0.9, 0.98]
      weight_decay: 0
  lr_scheduler:
    cls: torch.optim.lr_scheduler.StepLR
    kwargs:
      step_size: 50000
      gamma: 0.5
    unit: "step"
  trainer:
    unit: "step"
    min_steps: 0
    max_steps: 120000
    min_epochs: 0
    max_epochs: 1000
    val_every_n_units: 2000
    log_every_n_steps: 100
    num_sanity_val_steps: 1
    checkpoints:
      - tag: "best"
        type: "expression"
        expression: "total_loss"
        save_top_k: 5
        mode: "min"
        weights_only: false
      - tag: "temp"
        type: "periodic"
        unit: "step"
        every_n_units: 2000
        since_m_units: 0
        save_last_k: 2
        weights_only: false
      - tag: "perm"
        type: "periodic"
        unit: "step"
        every_n_units: 20000
        since_m_units: 80000
        save_last_k: -1
        weights_only: false
    precision: "16-mixed"
    accelerator: "auto"
    devices: "auto"
    num_nodes: 1
    strategy:
      name: "auto"
      kwargs:
        process_group_backend: nccl
        find_unused_parameters: false
    accumulate_grad_batches: 1
    gradient_clip_val: 1.0
  validation:
    use_vocoder: true
    spec_vmin: -14.
    spec_vmax: 4.
    max_plots: 10
  finetuning:
    pretraining_enabled: false
    pretraining_from: null
    pretraining_include_params: ["model.*"]
    pretraining_exclude_params: []
    freezing_enabled: false
    frozen_params: []
  weight_averaging:
    ema_enabled: false
    ema_decay: 0.999
    ema_include_params: ["model.*"]
    ema_exclude_params: []

inference:
  vocoder:
    vocoder_type: "nsf-hifigan"
    vocoder_path: "checkpoints/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02/model.ckpt"
